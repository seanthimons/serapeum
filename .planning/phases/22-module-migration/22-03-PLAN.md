---
phase: 22-module-migration
plan: 03
type: execute
wave: 2
depends_on: ["22-01"]
files_modified:
  - R/mod_search_notebook.R
autonomous: true

must_haves:
  truths:
    - "Search notebook embed handler writes to per-notebook ragnar store, not shared store"
    - "User removes paper from search results and its chunks are deleted from ragnar store"
    - "Search notebook with embedded abstracts but no per-notebook store shows migration prompt"
    - "User can cancel re-index mid-way via Stop button; partial store is deleted and rag_ready set to FALSE"
    - "Re-index progress shows per-paper detail (e.g. 'Embedding 3 of 12: Smith et al. 2023')"
    - "After embedding, chunks table rows have 'ragnar_indexed' sentinel in embedding column"
    - "Incremental embedding appends to existing store without rebuild"
    - "Empty search notebook does not show migration prompt"
    - "Synthesis and chat send buttons are disabled with tooltip when rag_ready is FALSE"
    - "Synthesis and chat send handlers have early-return guards when rag_available is FALSE"
  artifacts:
    - path: "R/mod_search_notebook.R"
      provides: "Per-notebook store wiring for embed, chunk deletion on paper remove, migration prompt"
      contains: "ensure_ragnar_store"
  key_links:
    - from: "R/mod_search_notebook.R"
      to: "R/_ragnar.R"
      via: "ensure_ragnar_store, delete_abstract_chunks_from_ragnar, mark_as_ragnar_indexed, get_notebook_ragnar_path"
      pattern: "delete_abstract_chunks_from_ragnar|ensure_ragnar_store"
---

<objective>
Migrate search notebook module to use per-notebook ragnar stores: rewire embed handler, add chunk deletion on paper removal, add migration prompt for existing notebooks, mark embedded chunks with sentinel value.

Purpose: Search notebooks achieve per-notebook isolation. Embedding, retrieval, and paper removal all operate on the per-notebook store. Incremental embedding appends without rebuild.

Output: Updated mod_search_notebook.R with full per-notebook store wiring.
</objective>

<execution_context>
@C:/Users/sxthi/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/sxthi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/22-module-migration/22-RESEARCH.md
@.planning/phases/22-module-migration/22-01-SUMMARY.md
@.planning/phases/21-store-lifecycle/21-02-SUMMARY.md

Key references:
- R/mod_search_notebook.R lines 1671-1780: Embed handler (currently uses shared store via get_ragnar_store)
- R/mod_search_notebook.R lines 1715-1720: Shared store path construction
- R/mod_search_notebook.R lines 582-604: papers_need_embedding reactive (checks chunks.embedding IS NOT NULL)
- R/mod_search_notebook.R lines 609-636: embed_button renderUI
- R/mod_search_notebook.R lines 695-718: Paper delete handler (adds to exclusion list, deletes from DB)
- R/_ragnar.R: ensure_ragnar_store(), delete_abstract_chunks_from_ragnar(), mark_as_ragnar_indexed(), get_notebook_ragnar_path()
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add migration detection and rag_ready state to search notebook</name>
  <files>R/mod_search_notebook.R</files>
  <action>
Add per-notebook store migration support to `mod_search_notebook.R`:

**1. Add reactive state variables** (in the module server function, near the top where other reactiveVals are declared):
```r
# Phase 22: Per-notebook store migration state
rag_ready <- reactiveVal(TRUE)
store_healthy <- reactiveVal(NULL)
```

**2. Add notebook-open migration check** (as a new observeEvent on notebook_id):

Add near the beginning of the server function. This mirrors the document notebook pattern but checks abstracts instead of documents:

```r
# Phase 22: Check for per-notebook store migration on notebook open
observeEvent(notebook_id(), {
  nb_id <- notebook_id()
  req(nb_id)

  # Reset state
  rag_ready(TRUE)
  store_healthy(NULL)

  store_path <- get_notebook_ragnar_path(nb_id)

  # Check if notebook has embedded content but no per-notebook store
  has_content <- tryCatch({
    abstracts <- DBI::dbGetQuery(con(), "
      SELECT COUNT(*) as cnt FROM abstracts WHERE notebook_id = ?
    ", list(nb_id))
    abstracts$cnt[1] > 0
  }, error = function(e) FALSE)

  if (has_content && !file.exists(store_path)) {
    # Has abstracts but no per-notebook store — needs migration
    rag_ready(FALSE)
    showModal(modalDialog(
      title = "Search Index Setup Required",
      tags$p("This notebook has papers but no search index. Synthesis and chat features will be unavailable until you re-index."),
      footer = tagList(
        actionButton(ns("reindex_search_nb"), "Re-index Now", class = "btn-primary"),
        modalButton("Later")
      ),
      easyClose = FALSE
    ))
  } else if (file.exists(store_path)) {
    # Store exists — check integrity
    result <- check_store_integrity(store_path)
    store_healthy(result$ok)
    rag_ready(result$ok)
    if (!result$ok) {
      showModal(modalDialog(
        title = "Search Index Needs Rebuild",
        tags$p("The search index for this notebook appears to be corrupted."),
        tags$p(class = "text-muted small", paste("Error:", result$error)),
        footer = tagList(
          actionButton(ns("rebuild_search_index"), "Rebuild Index", class = "btn-primary"),
          modalButton("Later")
        ),
        easyClose = FALSE
      ))
    }
  } else {
    # No content, no store — fine, lazy creation later
    store_healthy(TRUE)
    rag_ready(TRUE)
  }
})
```

**3. Add async re-index infrastructure** (reactive state + ExtendedTask, same pattern as mod_document_notebook / mod_citation_network):

Add reactive state variables for async re-indexing (near rag_ready declaration):
```r
current_interrupt_flag <- reactiveVal(NULL)
current_progress_file <- reactiveVal(NULL)
reindex_poller <- reactiveVal(NULL)
```

Add an ExtendedTask for async re-indexing:
```r
reindex_task <- ExtendedTask$new(function(notebook_id, db_path, api_key, embed_model, interrupt_flag, progress_file, app_dir) {
  mirai::mirai({
    source(file.path(app_dir, "R", "interrupt.R"), local = TRUE)
    source(file.path(app_dir, "R", "_ragnar.R"), local = TRUE)
    source(file.path(app_dir, "R", "db.R"), local = TRUE)

    result <- rebuild_notebook_store(
      notebook_id = notebook_id,
      con = NULL,
      db_path = db_path,
      api_key = api_key,
      embed_model = embed_model,
      interrupt_flag = interrupt_flag,
      progress_file = progress_file,
      progress_callback = NULL
    )
    result
  }, notebook_id = notebook_id, db_path = db_path, api_key = api_key,
     embed_model = embed_model, interrupt_flag = interrupt_flag,
     progress_file = progress_file, app_dir = app_dir)
})
```

**4. Add re-index handler** (launches async task with progress modal and Stop button):
```r
observeEvent(input$reindex_search_nb, {
  removeModal()
  nb_id <- notebook_id()
  req(nb_id)

  cfg <- config()
  api_key <- get_setting(cfg, "openrouter", "api_key")
  embed_model <- get_setting(cfg, "defaults", "embedding_model") %||% "openai/text-embedding-3-small"
  db_path <- get_setting(cfg, "app", "db_path") %||% "data/notebooks.duckdb"

  # Create interrupt and progress files
  flag_file <- create_interrupt_flag(session$token)
  progress_file <- create_progress_file(session$token)
  current_interrupt_flag(flag_file)
  current_progress_file(progress_file)

  # Show progress modal with Stop button
  showModal(modalDialog(
    title = "Re-indexing Search Notebook",
    div(
      div(id = ns("reindex_message"), "Initializing..."),
      div(class = "progress mt-2",
        div(class = "progress-bar progress-bar-striped progress-bar-animated",
            id = ns("reindex_bar"), role = "progressbar",
            style = "width: 0%", `aria-valuenow` = "0",
            `aria-valuemin` = "0", `aria-valuemax` = "100")
      )
    ),
    footer = actionButton(ns("cancel_reindex"), "Stop", class = "btn-warning"),
    easyClose = FALSE
  ))

  # Start progress poller
  poller <- observe({
    invalidateLater(1000)
    prog <- read_reindex_progress(current_progress_file())
    session$sendCustomMessage("updateSearchReindexProgress", list(
      bar_id = ns("reindex_bar"),
      msg_id = ns("reindex_message"),
      pct = prog$pct,
      message = prog$message
    ))
  })
  reindex_poller(poller)

  # Launch async task
  reindex_task$invoke(nb_id, db_path, api_key, embed_model, flag_file, progress_file, getwd())
})
```

**5. Add cancel handler:**
```r
observeEvent(input$cancel_reindex, {
  flag <- current_interrupt_flag()
  if (!is.null(flag)) signal_interrupt(flag)

  poller <- reindex_poller()
  if (!is.null(poller)) poller$destroy()
  reindex_poller(NULL)

  showModal(modalDialog(
    title = "Stopping Re-index",
    tags$p("Cancelling... please wait for current item to finish."),
    footer = NULL,
    easyClose = FALSE
  ))
})
```

**6. Add task result handler:**
```r
observe({
  result <- reindex_task$result()

  poller <- reindex_poller()
  if (!is.null(poller)) poller$destroy()
  reindex_poller(NULL)

  clear_interrupt_flag(current_interrupt_flag())
  clear_progress_file(current_progress_file())
  current_interrupt_flag(NULL)
  current_progress_file(NULL)

  removeModal()

  if (isTRUE(result$partial)) {
    # Cancelled — delete partial store
    tryCatch(delete_notebook_store(notebook_id()), error = function(e) NULL)
    rag_ready(FALSE)
    store_healthy(FALSE)
    showNotification("Re-indexing cancelled. Partial index removed.", type = "warning", duration = 5)
  } else if (isTRUE(result$success)) {
    rag_ready(TRUE)
    store_healthy(TRUE)
    tryCatch({
      abstract_ids <- DBI::dbGetQuery(con(), "SELECT id FROM abstracts WHERE notebook_id = ?", list(notebook_id()))$id
      mark_as_ragnar_indexed(con(), abstract_ids, source_type = "abstract")
    }, error = function(e) message("[ragnar] Sentinel update failed: ", e$message))
    showNotification(paste("Re-indexed", result$count, "items successfully."), type = "message", duration = 5)
  } else {
    rag_ready(FALSE)
    store_healthy(FALSE)
    showNotification(paste("Re-indexing failed:", result$error), type = "error", duration = NULL)
  }
})
```

**7. Add JS handler for progress bar updates** (in module UI):
```r
tags$script(HTML(sprintf("
  Shiny.addCustomMessageHandler('updateSearchReindexProgress', function(data) {
    var bar = document.getElementById(data.bar_id);
    var msg = document.getElementById(data.msg_id);
    if (bar) {
      bar.style.width = data.pct + '%%';
      bar.setAttribute('aria-valuenow', data.pct);
    }
    if (msg) msg.textContent = data.message;
  });
")))
```

**8. Rebuild handler for corruption recovery** (same async pattern — reuse the same ExtendedTask):
```r
observeEvent(input$rebuild_search_index, {
  removeModal()
  # Same pattern as reindex_search_nb handler above — invoke reindex_task
  # with the same parameters. The ExtendedTask handles both migration and corruption recovery.
  nb_id <- notebook_id()
  req(nb_id)
  cfg <- config()
  api_key <- get_setting(cfg, "openrouter", "api_key")
  embed_model <- get_setting(cfg, "defaults", "embedding_model") %||% "openai/text-embedding-3-small"
  db_path <- get_setting(cfg, "app", "db_path") %||% "data/notebooks.duckdb"

  flag_file <- create_interrupt_flag(session$token)
  progress_file <- create_progress_file(session$token)
  current_interrupt_flag(flag_file)
  current_progress_file(progress_file)

  showModal(modalDialog(
    title = "Rebuilding Search Index",
    div(
      div(id = ns("reindex_message"), "Initializing rebuild..."),
      div(class = "progress mt-2",
        div(class = "progress-bar progress-bar-striped progress-bar-animated",
            id = ns("reindex_bar"), role = "progressbar",
            style = "width: 0%")
      )
    ),
    footer = actionButton(ns("cancel_reindex"), "Stop", class = "btn-warning"),
    easyClose = FALSE
  ))

  poller <- observe({
    invalidateLater(1000)
    prog <- read_reindex_progress(current_progress_file())
    session$sendCustomMessage("updateSearchReindexProgress", list(
      bar_id = ns("reindex_bar"), msg_id = ns("reindex_message"),
      pct = prog$pct, message = prog$message
    ))
  })
  reindex_poller(poller)

  reindex_task$invoke(nb_id, db_path, api_key, embed_model, flag_file, progress_file, getwd())
})
```

**4. Wire rag_ready to block synthesis and chat when store is unavailable:**

The embed button itself should remain functional (embedding IS the mechanism to build the store for new papers). However, synthesis and chat features must be guarded:

**4a. Add rag_available reactive** (near rag_ready/store_healthy declarations):
```r
rag_available <- reactive({
  isTRUE(store_healthy()) && isTRUE(rag_ready())
})
```

**4b. Add early-return guard in the chat/synthesis send handler(s).** In the search notebook, find the `observeEvent(input$send_synthesis, {...})` or equivalent synthesis trigger handler and add at the top:
```r
# Phase 22: Block synthesis when per-notebook store is unavailable
if (!isTRUE(rag_available())) {
  showNotification("Synthesis unavailable \u2014 re-index this notebook first.", type = "warning")
  return()
}
```

Similarly, if there is a chat send handler in the search notebook (`observeEvent(input$send_chat, {...})` or similar), add the same guard:
```r
if (!isTRUE(rag_available())) {
  showNotification("Chat unavailable \u2014 re-index this notebook first.", type = "warning")
  return()
}
```

**4c. Visually disable the synthesis/chat send button(s)** when rag_available is FALSE. Wrap the send button in a `renderUI` that checks `rag_available()`:
```r
output$synthesis_send_ui <- renderUI({
  if (isTRUE(rag_available())) {
    actionButton(ns("send_synthesis"), "Synthesize", class = "btn-primary")
  } else {
    tags$button(
      class = "btn btn-primary disabled",
      disabled = "disabled",
      title = "Synthesis unavailable \u2014 re-index this notebook first",
      "Synthesize"
    )
  }
})
```
Replace the static synthesis button in the UI function with `uiOutput(ns("synthesis_send_ui"))`. Apply the same pattern to any chat send button in the search notebook.
  </action>
  <verify>
Run: `"C:\Program Files\R\R-4.5.1\bin\Rscript.exe" -e "parse('R/mod_search_notebook.R'); cat('mod_search_notebook.R parses OK\n')"`
Grep for "rag_ready" in R/mod_search_notebook.R — should find declaration and usage.
Grep for "reindex_search_nb" in R/mod_search_notebook.R — should find input binding and handler.
Grep for "ExtendedTask" in R/mod_search_notebook.R — should find async task declaration.
Grep for "mirai" in R/mod_search_notebook.R — should find mirai call in ExtendedTask.
Grep for "cancel_reindex" in R/mod_search_notebook.R — should find cancel button and handler.
Grep for "signal_interrupt" in R/mod_search_notebook.R — should find interrupt signaling.
Grep for "read_reindex_progress" in R/mod_search_notebook.R — should find progress polling.
Grep for "store_healthy" in R/mod_search_notebook.R — should find declaration and usage.
Grep for "rag_available" in R/mod_search_notebook.R — should find reactive and usage in send handlers.
Grep for "synthesis_send_ui" in R/mod_search_notebook.R — should find renderUI for disabled button.
  </verify>
  <done>Search notebook module detects migration need and shows async cancellable re-index with per-paper detail progress (e.g. "Embedding 3 of 12: Smith et al. 2023"). Cancellation deletes partial store. Toast notification on success. rag_ready and store_healthy states track availability. Synthesis/chat send buttons are visually disabled when rag_available is FALSE, with early-return guards as defense in depth. Rebuild handler recovers from corruption using same async pattern.</done>
</task>

<task type="auto">
  <name>Task 2: Rewire embed handler to per-notebook store and add chunk deletion on paper removal</name>
  <files>R/mod_search_notebook.R</files>
  <action>
**Part A: Rewire the embed handler (lines 1671-1780)**

In the `observeEvent(input$embed_papers, {...})` handler:

1. Replace the shared store path construction (lines 1715-1720):
```r
# OLD (remove):
ragnar_store_path <- file.path(
  dirname(get_setting(cfg, "app", "db_path") %||% "data/notebooks.duckdb"),
  "serapeum.ragnar.duckdb")
store <- get_ragnar_store(ragnar_store_path,
                          openrouter_api_key = api_key_or,
                          embed_model = embed_model)
```

With per-notebook store:
```r
# Phase 22: Use per-notebook ragnar store
store <- tryCatch(
  ensure_ragnar_store(nb_id, session, api_key_or, embed_model),
  error = function(e) {
    message("[ragnar] Failed to open per-notebook store: ", e$message)
    store_healthy(FALSE)
    NULL
  }
)
```

2. Keep the `ragnar_available()` check on line 1702 as-is (will be removed in Phase 23).

3. After `build_ragnar_index(store)` succeeds and `ragnar_indexed <- TRUE` (around line 1738), add sentinel marking:
```r
# Mark embedded abstracts in chunks table (sentinel for "needs embedding" detection)
tryCatch({
  mark_as_ragnar_indexed(con(), paper_ids, source_type = "abstract")
}, error = function(e) message("[ragnar] Sentinel marking failed: ", e$message))

# Update rag state
rag_ready(TRUE)
store_healthy(TRUE)
```

4. Update the origin encoding in the abstract chunk data.frame (line 1729). Currently it uses:
```r
origin = paste0("abstract:", abs_row$id)
```
This is already correct for the `delete_abstract_chunks_from_ragnar` function which matches by `abstract:{id}` prefix. Keep it as-is. However, to support section_hint metadata for future filtering, enhance it to use `encode_origin_metadata`:
```r
origin = encode_origin_metadata(
  paste0("abstract:", abs_row$id),
  section_hint = "general",
  doi = NULL,
  source_type = "abstract"
)
```
This produces: `"abstract:{id}|section=general|type=abstract"` — enabling future section filtering. The delete function uses LIKE prefix matching so `abstract:{id}%` will still match.

**Part B: Add chunk deletion on paper removal**

In the paper delete handler (around lines 695-718), after `delete_abstract(con(), paper$id)` (line 712), add:

```r
# Phase 22: Delete chunks from per-notebook ragnar store
tryCatch({
  delete_abstract_chunks_from_ragnar(notebook_id(), paper$id)
}, error = function(e) {
  message("[ragnar] Failed to delete chunks for removed paper: ", e$message)
})
```

This should be placed AFTER the `delete_abstract()` call but BEFORE `paper_refresh(paper_refresh() + 1)`.

**Part C: Update papers_need_embedding reactive** (lines 582-604)

The current check queries `chunks.embedding IS NOT NULL`. After Phase 22, embedded papers have `embedding = 'ragnar_indexed'` (not NULL). This check already works correctly because `'ragnar_indexed' IS NOT NULL` evaluates to TRUE — so papers with the sentinel will correctly NOT appear in the "needs embedding" count. No change needed.

However, verify this by reading the exact query. The query at line 596-602 counts chunks where `embedding IS NOT NULL`. Papers with `'ragnar_indexed'` sentinel will have `embedding IS NOT NULL` = TRUE, so they'll be counted as embedded. Correct.
  </action>
  <verify>
Run: `"C:\Program Files\R\R-4.5.1\bin\Rscript.exe" -e "parse('R/mod_search_notebook.R'); cat('mod_search_notebook.R parses OK\n')"`
Grep for "serapeum.ragnar.duckdb" in R/mod_search_notebook.R — should return zero results.
Grep for "ensure_ragnar_store" in R/mod_search_notebook.R — should find call in embed handler.
Grep for "delete_abstract_chunks_from_ragnar" in R/mod_search_notebook.R — should find call in delete handler.
Grep for "mark_as_ragnar_indexed" in R/mod_search_notebook.R — should find call after embed.
Grep for "encode_origin_metadata" in R/mod_search_notebook.R — should find call in chunk data.frame.
  </verify>
  <done>Search notebook embed handler uses per-notebook store. Paper removal deletes chunks from ragnar. Sentinel marking tracks embedding status. Origin encoding includes section metadata. No shared store references remain.</done>
</task>

</tasks>

<verification>
1. R/mod_search_notebook.R parses without errors
2. No "serapeum.ragnar.duckdb" references remain
3. Embed handler uses ensure_ragnar_store() for per-notebook store
4. Paper deletion triggers delete_abstract_chunks_from_ragnar()
5. After embedding, mark_as_ragnar_indexed() sets sentinel values
6. Origin encoding uses encode_origin_metadata for section_hint support
</verification>

<success_criteria>
- Embedding writes to per-notebook ragnar store (data/ragnar/{notebook_id}.duckdb)
- Removing paper from search results deletes its chunks from ragnar store
- Migration prompt shows for search notebooks with content but no per-notebook store
- Re-index uses ExtendedTask + mirai async pattern with cancellation support
- Progress modal shows per-paper detail (paper title in progress messages)
- Cancel button signals interrupt; partial store is deleted on cancellation
- Success shows toast notification
- Incremental embedding appends to existing store
- papers_need_embedding count correctly identifies unembedded papers
- Synthesis/chat send buttons disabled with tooltip when rag_ready is FALSE
- Synthesis/chat send handlers guard with early-return when rag_available is FALSE
</success_criteria>

<output>
After completion, create `.planning/phases/22-module-migration/22-03-SUMMARY.md`
</output>
