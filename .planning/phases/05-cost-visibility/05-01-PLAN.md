---
phase: 05-cost-visibility
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - R/api_openrouter.R
  - R/db.R
  - R/cost_tracking.R
  - migrations/003_create_cost_log.sql
autonomous: true
user_setup: []

must_haves:
  truths:
    - "chat_completion() returns both content and usage metadata (tokens, model)"
    - "get_embeddings() returns both embeddings and usage metadata (tokens, model)"
    - "Cost records can be persisted to the database with operation type, model, tokens, and estimated cost"
    - "Session costs can be queried from the database"
    - "Historical costs can be queried by date range"
  artifacts:
    - path: "R/api_openrouter.R"
      provides: "chat_completion and get_embeddings return structured results with usage info"
      contains: "usage"
    - path: "R/cost_tracking.R"
      provides: "Cost logging and query functions"
      exports: ["log_cost", "get_session_costs", "get_cost_history", "estimate_cost"]
    - path: "migrations/003_create_cost_log.sql"
      provides: "cost_log table schema"
      contains: "CREATE TABLE"
    - path: "R/db.R"
      provides: "cost_log table in init_schema (no-op for migrated DBs)"
  key_links:
    - from: "R/api_openrouter.R"
      to: "R/cost_tracking.R"
      via: "usage metadata returned from API functions"
      pattern: "usage|prompt_tokens|completion_tokens"
    - from: "R/cost_tracking.R"
      to: "R/db.R"
      via: "DBI queries against cost_log table"
      pattern: "cost_log"
---

<objective>
Create the cost tracking backend: modify API functions to return usage metadata, create the cost_log database table, and build cost logging/query helper functions.

Purpose: All downstream UI and integration depends on having (a) cost data from API responses, and (b) a place to store and query it. This plan establishes both foundations.

Output: Modified api_openrouter.R, new cost_tracking.R, migration 003, updated db.R.
</objective>

<execution_context>
@C:/Users/sxthi/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/sxthi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@R/api_openrouter.R
@R/db.R
@R/db_migrations.R
@R/rag.R
@R/slides.R
@R/mod_query_builder.R
@R/mod_document_notebook.R
@R/_ragnar.R
</context>

<tasks>

<task type="auto">
  <name>Task 1: Modify API functions to return usage metadata and create cost helpers</name>
  <files>R/api_openrouter.R, R/cost_tracking.R</files>
  <action>
**In R/api_openrouter.R:**

1. Modify `chat_completion()` to return a list instead of just the content string:
   - Return `list(content = body$choices[[1]]$message$content, usage = body$usage, model = model, id = body$id)`
   - `body$usage` contains `prompt_tokens`, `completion_tokens`, `total_tokens` from OpenRouter
   - Keep error handling as-is

2. Modify `get_embeddings()` to return a list instead of just embedding vectors:
   - Return `list(embeddings = lapply(body$data, function(x) unlist(x$embedding)), usage = body$usage, model = model)`
   - `body$usage` may contain `prompt_tokens` and `total_tokens`

IMPORTANT: These are BREAKING changes to the return type. All callers will need updating (done in Plan 02). The old callers expect a string from chat_completion and a list of vectors from get_embeddings. Plan 02 will update every caller.

**Create R/cost_tracking.R:**

1. `estimate_cost(model, prompt_tokens, completion_tokens)` - Estimates USD cost from token counts. Use a lookup table of known OpenRouter model prices per token. Include at minimum these models (prices in USD per token):
   - `openai/gpt-4o-mini`: prompt=$0.15/M, completion=$0.60/M
   - `openai/gpt-4o`: prompt=$2.50/M, completion=$10.00/M
   - `google/gemini-2.0-flash-001`: prompt=$0.10/M, completion=$0.40/M
   - `google/gemini-2.5-flash-preview-05-20`: prompt=$0.15/M, completion=$0.60/M
   - `anthropic/claude-sonnet-4`: prompt=$3.00/M, completion=$15.00/M
   - `openai/text-embedding-3-small`: prompt=$0.02/M, completion=$0.00/M
   - `openai/text-embedding-3-large`: prompt=$0.13/M, completion=$0.00/M
   - For unknown models, use a conservative default: prompt=$1.00/M, completion=$3.00/M
   - Return numeric USD cost (e.g., 0.00045)

2. `log_cost(con, operation, model, prompt_tokens, completion_tokens, total_tokens, estimated_cost, session_id)` - Inserts a row into the cost_log table. Parameters:
   - `con`: DuckDB connection
   - `operation`: one of "chat", "embedding", "query_build", "slide_generation"
   - `model`: model ID string
   - `prompt_tokens`, `completion_tokens`, `total_tokens`: integers
   - `estimated_cost`: numeric USD
   - `session_id`: character string identifying the Shiny session (for grouping session totals)

3. `get_session_costs(con, session_id)` - Returns a data frame of all cost_log rows for the given session_id, plus a summary row with totals. Return columns: operation, model, prompt_tokens, completion_tokens, total_tokens, estimated_cost, created_at. Also return an attribute `total_cost` on the data frame.

4. `get_cost_history(con, days = 30)` - Returns daily aggregated costs for the last N days. Columns: date, total_cost, request_count, total_tokens. Uses SQL GROUP BY on date portion of created_at.

5. `get_cost_by_operation(con, days = 30)` - Returns cost aggregated by operation type for the last N days. Columns: operation, total_cost, request_count, avg_cost_per_request.
  </action>
  <verify>
Run in R console:
```r
source("R/cost_tracking.R")
# Test estimate_cost
estimate_cost("openai/gpt-4o-mini", 1000, 500)  # Should return ~0.00045
estimate_cost("unknown/model", 1000, 500)  # Should return default pricing

source("R/api_openrouter.R")
# Verify functions are defined (don't need API key to check structure)
is.function(chat_completion)  # TRUE
is.function(get_embeddings)   # TRUE
```
  </verify>
  <done>
- chat_completion() returns list with content + usage + model + id
- get_embeddings() returns list with embeddings + usage + model
- cost_tracking.R exports estimate_cost, log_cost, get_session_costs, get_cost_history, get_cost_by_operation
- estimate_cost returns correct USD values for known models
  </done>
</task>

<task type="auto">
  <name>Task 2: Create cost_log database table via migration</name>
  <files>migrations/003_create_cost_log.sql, R/db.R</files>
  <action>
**Create migrations/003_create_cost_log.sql:**

```sql
-- Cost tracking table for LLM API usage
CREATE TABLE IF NOT EXISTS cost_log (
  id VARCHAR PRIMARY KEY,
  session_id VARCHAR NOT NULL,
  operation VARCHAR NOT NULL,
  model VARCHAR NOT NULL,
  prompt_tokens INTEGER DEFAULT 0,
  completion_tokens INTEGER DEFAULT 0,
  total_tokens INTEGER DEFAULT 0,
  estimated_cost DOUBLE DEFAULT 0.0,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Index for session queries (get all costs for current session)
CREATE INDEX IF NOT EXISTS idx_cost_log_session ON cost_log(session_id);

-- Index for date-range history queries
CREATE INDEX IF NOT EXISTS idx_cost_log_date ON cost_log(created_at);
```

The `id` column uses UUID (generated in R by log_cost). The `session_id` groups costs per Shiny session. The `operation` column categorizes what triggered the cost (chat, embedding, query_build, slide_generation).

**In R/db.R**, add the cost_log table to `init_schema()` as a CREATE TABLE IF NOT EXISTS (for forward compatibility, same as other tables). Add it after the existing `quality_cache_meta` table creation:

```r
# Cost tracking table
dbExecute(con, "
  CREATE TABLE IF NOT EXISTS cost_log (
    id VARCHAR PRIMARY KEY,
    session_id VARCHAR NOT NULL,
    operation VARCHAR NOT NULL,
    model VARCHAR NOT NULL,
    prompt_tokens INTEGER DEFAULT 0,
    completion_tokens INTEGER DEFAULT 0,
    total_tokens INTEGER DEFAULT 0,
    estimated_cost DOUBLE DEFAULT 0.0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
  )
")
```

Do NOT add the indexes in init_schema (they'll come from migration). The init_schema addition is a safety net for fresh databases that somehow skip migrations.
  </action>
  <verify>
Run in R console:
```r
source("R/db_migrations.R")
source("R/db.R")
con <- get_db_connection("data/notebooks.duckdb")
# Check migration was applied
applied <- get_applied_migrations(con)
3 %in% applied  # Should be TRUE

# Check table exists
DBI::dbGetQuery(con, "SELECT * FROM cost_log LIMIT 0")  # Should return empty df with correct columns
DBI::dbDisconnect(con, shutdown = TRUE)
```
  </verify>
  <done>
- Migration 003 exists and creates cost_log table with indexes
- init_schema() has matching CREATE TABLE IF NOT EXISTS for cost_log
- Database at version 003 after connection
- cost_log table has columns: id, session_id, operation, model, prompt_tokens, completion_tokens, total_tokens, estimated_cost, created_at
  </done>
</task>

</tasks>

<verification>
1. `source("R/api_openrouter.R")` loads without error
2. `source("R/cost_tracking.R")` loads without error
3. Migration 003 file exists at `migrations/003_create_cost_log.sql`
4. Fresh DB connection applies migration 003 automatically
5. `estimate_cost()` returns correct values for known models
6. `log_cost()` can insert and `get_session_costs()` can retrieve
</verification>

<success_criteria>
- API functions return structured results with usage metadata
- cost_log table exists in database after migration
- Cost helper functions (log, query, estimate) are available
- No existing functionality is broken (callers will be updated in Plan 02)
</success_criteria>

<output>
After completion, create `.planning/phases/05-cost-visibility/05-01-SUMMARY.md`
</output>
